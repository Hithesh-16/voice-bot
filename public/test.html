<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Voice Bot – Pipeline (Architecture)</title>
  <style>
    :root {
      --bg: #0f0f12;
      --surface: #1a1a1f;
      --border: #2d2d35;
      --text: #e4e4e7;
      --muted: #71717a;
      --accent: #22c55e;
      --accent-hover: #16a34a;
      --error: #ef4444;
      --step-active: #22c55e33;
      --step-done: #22c55e22;
    }
    * { box-sizing: border-box; }
    body {
      font-family: 'Segoe UI', system-ui, sans-serif;
      background: var(--bg);
      color: var(--text);
      margin: 0;
      min-height: 100vh;
      padding: 1.5rem;
    }
    h1 { font-size: 1.35rem; margin: 0 0 0.25rem 0; }
    .sub { color: var(--muted); font-size: 0.875rem; margin-bottom: 1.25rem; }
    .config-bar {
      display: flex;
      gap: 1rem;
      align-items: center;
      margin-bottom: 1.5rem;
      flex-wrap: wrap;
    }
    .config-bar label { color: var(--muted); font-size: 0.8125rem; }
    .config-bar select {
      background: var(--surface);
      border: 1px solid var(--border);
      color: var(--text);
      padding: 0.4rem 0.6rem;
      border-radius: 6px;
      font-size: 0.875rem;
    }
    .voice-settings {
      background: var(--surface);
      border: 1px solid var(--border);
      border-radius: 10px;
      padding: 1rem 1.25rem;
      margin-bottom: 1.25rem;
      max-width: 560px;
      margin-left: auto;
      margin-right: auto;
    }
    .voice-settings-title { font-size: 0.8125rem; font-weight: 600; margin-bottom: 0.75rem; color: var(--muted); }
    .voice-settings-grid { display: grid; grid-template-columns: auto 1fr; gap: 0.5rem 1rem; align-items: center; }
    .voice-settings-grid label { font-size: 0.8125rem; color: var(--muted); }
    .voice-settings-grid select { background: var(--bg); border: 1px solid var(--border); color: var(--text); padding: 0.4rem 0.6rem; border-radius: 6px; font-size: 0.8125rem; min-width: 140px; }
    .pipeline {
      max-width: 560px;
      margin: 0 auto;
    }
    .step {
      background: var(--surface);
      border: 1px solid var(--border);
      border-radius: 10px;
      padding: 1rem 1.25rem;
      margin-bottom: 0.75rem;
      transition: background 0.2s, border-color 0.2s;
    }
    .step.active { border-color: var(--accent); background: var(--step-active); }
    .step.done { border-color: var(--accent); background: var(--step-done); }
    .step-title {
      font-size: 0.75rem;
      text-transform: uppercase;
      letter-spacing: 0.05em;
      color: var(--muted);
      margin-bottom: 0.35rem;
    }
    .step-name { font-size: 0.9375rem; font-weight: 600; margin-bottom: 0.5rem; }
    .step-value {
      font-size: 0.875rem;
      min-height: 1.5em;
      white-space: pre-wrap;
      word-break: break-word;
      color: var(--muted);
    }
    .step-value.has-value { color: var(--text); }
    .step-value.status { font-style: italic; }
    .step-hint { font-size: 0.75rem; color: var(--muted); margin-top: 0.5rem; }
    .step-actions { margin-top: 0.5rem; display: flex; gap: 0.5rem; flex-wrap: wrap; }
    button {
      background: var(--accent);
      color: #000;
      border: none;
      padding: 0.5rem 1rem;
      border-radius: 8px;
      font-size: 0.8125rem;
      font-weight: 600;
      cursor: pointer;
    }
    button:hover { background: var(--accent-hover); }
    button:disabled { opacity: 0.5; cursor: not-allowed; }
    button.secondary { background: var(--border); color: var(--text); }
    button.secondary:hover:not(:disabled) { background: #3d3d48; }
    .connector { text-align: center; color: var(--muted); font-size: 0.75rem; margin: 0.15rem 0; }
    .log { margin-top: 1.5rem; padding: 1rem; background: var(--surface); border: 1px solid var(--border); border-radius: 8px; font-size: 0.75rem; max-height: 180px; overflow-y: auto; white-space: pre-wrap; }
    .log .err { color: var(--error); }
    .log .bot { color: var(--accent); }
    .log .meta { color: var(--muted); }
    audio { width: 100%; margin-top: 0.5rem; height: 32px; }
  </style>
</head>
<body>
  <h1>Voice bot pipeline</h1>
  <p class="sub">Flow matches architecture: Voice → Orchestration → STT → AI Brain → TTS. Each step updates as data flows.</p>

  <div class="config-bar">
    <span><label>Business type</label> <select id="vertical"><option value="">Loading…</option></select></span>
    <span><label>Provider</label> <select id="provider"><option value="">Loading…</option></select></span>
    <span><label>Model</label> <select id="model"><option value="">—</option></select></span>
  </div>

  <div class="voice-settings">
    <div class="voice-settings-title">Voice response (Deepgram TTS)</div>
    <div class="voice-settings-grid">
      <label>Speed</label>
      <select id="ttsSpeed">
        <option value="0.75">0.75× slower</option>
        <option value="0.9">0.9×</option>
        <option value="1" selected>1× normal</option>
        <option value="1.1">1.1×</option>
        <option value="1.25">1.25×</option>
        <option value="1.5">1.5× faster</option>
      </select>
      <label>Voice (model)</label>
      <select id="ttsModel">
        <option value="aura-2-thalia-en">Thalia (clear, confident)</option>
        <option value="aura-2-asteria-en">Asteria (clear, energetic)</option>
        <option value="aura-2-helena-en">Helena (caring, friendly)</option>
        <option value="aura-2-apollo-en">Apollo (confident, casual)</option>
        <option value="aura-2-arcas-en">Arcas (smooth, clear)</option>
        <option value="aura-2-luna-en">Luna (friendly, natural)</option>
        <option value="aura-2-zeus-en">Zeus (deep, trustworthy)</option>
        <option value="aura-2-electra-en">Electra (professional)</option>
        <option value="aura-asteria-en">Asteria (Aura 1)</option>
      </select>
      <label>Encoding</label>
      <select id="ttsEncoding">
        <option value="mp3" selected>MP3</option>
        <option value="linear16">Linear16</option>
        <option value="opus">Opus</option>
        <option value="flac">FLAC</option>
        <option value="aac">AAC</option>
      </select>
      <label>Sample rate</label>
      <select id="ttsSampleRate">
        <option value="">Default</option>
        <option value="8000">8000 Hz</option>
        <option value="16000">16000 Hz</option>
        <option value="24000">24000 Hz</option>
        <option value="32000">32000 Hz</option>
        <option value="48000">48000 Hz</option>
      </select>
      <label>Bit rate</label>
      <select id="ttsBitRate">
        <option value="">Default</option>
        <option value="32000">32 kbps</option>
        <option value="48000">48 kbps</option>
      </select>
    </div>
  </div>

  <div class="pipeline">
    <!-- 1. Voice input (Telephony layer in prod = Twilio; here = Mic) -->
    <div class="step" id="step1">
      <div class="step-title">Step 1</div>
      <div class="step-name">Voice input (Telephony / Mic)</div>
      <div class="step-value status" id="step1Value">Idle — start listening to send audio</div>
      <div class="step-actions">
        <button id="startVoiceBtn">Start listening</button>
        <button id="stopVoiceBtn" class="secondary" disabled>Stop listening</button>
      </div>
    </div>
    <div class="connector">▼ WebSocket stream</div>

    <!-- 2. Voice orchestration -->
    <div class="step" id="step2">
      <div class="step-title">Step 2</div>
      <div class="step-name">Voice orchestration</div>
      <div class="step-value status" id="step2Value">Disconnected</div>
    </div>
    <div class="connector">▼ Audio chunks</div>

    <!-- 3. Speech-to-Text (Deepgram) -->
    <div class="step" id="step3">
      <div class="step-title">Step 3</div>
      <div class="step-name">Speech-to-Text (Deepgram)</div>
      <div class="step-value" id="step3Value">—</div>
      <div class="step-hint">Brief pauses are fine. Only after ~6s silence is your full sentence sent to the AI.</div>
    </div>
    <div class="connector">▼ After pause or sentence complete → sent to AI Brain</div>

    <!-- 4. AI Sales Brain (LLM) -->
    <div class="step" id="step4">
      <div class="step-title">Step 4</div>
      <div class="step-name">AI Sales Brain (LLM)</div>
      <div class="step-value" id="step4Value">Idle — your words (after pause) go here, then reply → TTS</div>
    </div>
    <div class="connector">▼ AI reply text → TTS (voice)</div>

    <!-- 5. Text-to-Speech -->
    <div class="step" id="step5">
      <div class="step-title">Step 5</div>
      <div class="step-name">Text-to-Speech</div>
      <div class="step-value" id="step5Value">Idle</div>
      <audio id="ttsAudio" controls style="display:none"></audio>
    </div>
  </div>

  <div class="log" id="log">Events will appear here.</div>

  <script>
    const verticalEl = document.getElementById('vertical');
    const providerEl = document.getElementById('provider');
    const modelEl = document.getElementById('model');
    const startVoiceBtn = document.getElementById('startVoiceBtn');
    const stopVoiceBtn = document.getElementById('stopVoiceBtn');
    const step1 = document.getElementById('step1');
    const step2 = document.getElementById('step2');
    const step3 = document.getElementById('step3');
    const step4 = document.getElementById('step4');
    const step5 = document.getElementById('step5');
    const step1Value = document.getElementById('step1Value');
    const step2Value = document.getElementById('step2Value');
    const step3Value = document.getElementById('step3Value');
    const step4Value = document.getElementById('step4Value');
    const step5Value = document.getElementById('step5Value');
    const ttsAudio = document.getElementById('ttsAudio');
    const logEl = document.getElementById('log');

    let providersData = { providers: [], defaultProvider: 'groq' };
    let voiceWs = null;
    let audioContext = null;
    let stream = null;
    let processor = null;
    let source = null;
    let conversationHistory = [];
    let busyWithResponse = false; // ignore new finals while brain/TTS is in progress (one turn at a time)
    const PAUSE_BEFORE_BRAIN_MS = 1200; // pause after user stops before sending to AI

    function log(msg, type = '') {
      const span = document.createElement('span');
      if (type) span.className = type;
      span.textContent = msg;
      logEl.appendChild(span);
      logEl.appendChild(document.createTextNode('\n'));
      logEl.scrollTop = logEl.scrollHeight;
    }

    function setStep(el, valueEl, value, state) {
      valueEl.textContent = value || '—';
      valueEl.classList.toggle('has-value', !!value && value !== '—');
      valueEl.classList.toggle('status', state === 'status');
      el.classList.remove('active', 'done');
      if (state === 'active') el.classList.add('active');
      if (state === 'done') el.classList.add('done');
    }

    function setVoiceButtons(listening) {
      startVoiceBtn.disabled = listening;
      stopVoiceBtn.disabled = !listening;
    }

    async function onFinalTranscript(transcript) {
      if (!transcript.trim()) return;
      if (busyWithResponse) return; // only one turn at a time; ignore late or extra finals
      busyWithResponse = true;
      setStep(step4, step4Value, 'Pause — sending to AI…', 'active');
      log('User (STT): ' + transcript, 'meta');
      await new Promise((r) => setTimeout(r, PAUSE_BEFORE_BRAIN_MS));
      setStep(step4, step4Value, 'Thinking…', 'active');
      conversationHistory.push({ role: 'user', content: transcript });
      try {
        const r = await fetch('/test/brain', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            messages: conversationHistory,
            vertical: verticalEl.value || undefined,
            provider: providerEl.value || undefined,
            model: modelEl.value || undefined,
          }),
        });
        const data = await r.json();
        if (!r.ok) {
          setStep(step4, step4Value, 'Error: ' + (data.error || r.status), 'status');
          log('Brain error: ' + (data.error || r.status), 'err');
          busyWithResponse = false;
          return;
        }
        const reply = data.reply || '';
        conversationHistory.push({ role: 'assistant', content: reply });
        setStep(step4, step4Value, reply, 'done');
        log('Bot: ' + reply, 'bot');

        setStep(step5, step5Value, 'Synthesizing…', 'active');
        const params = new URLSearchParams({ text: reply.slice(0, 4096) });
        const model = document.getElementById('ttsModel').value;
        const encoding = document.getElementById('ttsEncoding').value;
        const sampleRate = document.getElementById('ttsSampleRate').value;
        const bitRate = document.getElementById('ttsBitRate').value;
        if (model) params.set('model', model);
        if (encoding) params.set('encoding', encoding);
        if (sampleRate) params.set('sample_rate', sampleRate);
        if (bitRate) params.set('bit_rate', bitRate);
        const ttsUrl = '/test/tts?' + params.toString();
        const audioRes = await fetch(ttsUrl);
        if (!audioRes.ok) {
          setStep(step5, step5Value, 'TTS failed', 'status');
          busyWithResponse = false;
          return;
        }
        const blob = await audioRes.blob();
        const url = URL.createObjectURL(blob);
        ttsAudio.src = url;
        ttsAudio.style.display = 'block';
        const speed = parseFloat(document.getElementById('ttsSpeed').value) || 1;
        ttsAudio.playbackRate = speed;
        setStep(step5, step5Value, 'Playing… (use player below)', 'active');
        ttsAudio.onended = () => {
          setStep(step5, step5Value, 'Done. Speak again for next turn.', 'done');
          busyWithResponse = false;
        };
        ttsAudio.play();
      } catch (e) {
        setStep(step4, step4Value, 'Error: ' + e.message, 'status');
        setStep(step5, step5Value, 'Idle', 'status');
        log('Error: ' + e.message, 'err');
        busyWithResponse = false;
      }
    }

    function stopVoice() {
      setVoiceButtons(false);
      setStep(step1, step1Value, 'Idle — start listening to send audio', 'status');
      setStep(step2, step2Value, 'Disconnected', 'status');
      if (processor && source) {
        try { source.disconnect(); processor.disconnect(); } catch (_) {}
      }
      if (stream) stream.getTracks().forEach(t => t.stop());
      stream = null;
      if (audioContext) audioContext.close();
      audioContext = null;
      if (voiceWs && voiceWs.readyState === WebSocket.OPEN) voiceWs.close();
      voiceWs = null;
    }

    function fillModelDropdown(providerId) {
      const p = providersData.providers.find(x => x.id === providerId);
      modelEl.innerHTML = '';
      if (!p || !p.models?.length) {
        modelEl.appendChild(new Option('—', ''));
        return;
      }
      p.models.forEach(m => {
        const opt = new Option(m.name, m.id);
        if (m.id === p.defaultModel) opt.selected = true;
        modelEl.appendChild(opt);
      });
    }

    async function loadModels() {
      try {
        const r = await fetch('/test/models');
        const data = await r.json();
        providersData = data;
        providerEl.innerHTML = '';
        if (!data.providers?.length) {
          providerEl.appendChild(new Option('No provider (set GROQ_API_KEY)', ''));
          return;
        }
        data.providers.forEach(p => {
          const opt = new Option(p.name, p.id);
          if (p.id === (data.defaultProvider || 'groq')) opt.selected = true;
          providerEl.appendChild(opt);
        });
        fillModelDropdown(providerEl.value);
      } catch (e) {
        providerEl.innerHTML = '<option value="">Failed</option>';
      }
    }
    providerEl.addEventListener('change', () => fillModelDropdown(providerEl.value));

    async function loadVerticals() {
      try {
        const r = await fetch('/test/verticals');
        const data = await r.json();
        verticalEl.innerHTML = '';
        (data.verticals || []).forEach((v) => {
          const opt = new Option(v.name, v.id);
          if (v.id === (data.defaultVertical || 'sales')) opt.selected = true;
          verticalEl.appendChild(opt);
        });
      } catch (_) {
        verticalEl.innerHTML = '<option value="sales">Sales</option>';
      }
    }
    loadVerticals();
    loadModels();

    const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
    const wsUrl = wsProtocol + '//' + location.host + '/test/voice';

    startVoiceBtn.addEventListener('click', async () => {
      try {
        stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      } catch (e) {
        setStep(step1, step1Value, 'Microphone access denied.', 'status');
        return;
      }
      setVoiceButtons(true);
      setStep(step1, step1Value, 'Listening…', 'active');
      setStep(step2, step2Value, 'Connecting…', 'active');

      voiceWs = new WebSocket(wsUrl);
      voiceWs.binaryType = 'arraybuffer';
      voiceWs.onopen = () => {
        setStep(step2, step2Value, 'Connected — streaming audio', 'active');
        setStep(step3, step3Value, 'Listening… (finish your full sentence, then stay silent ~6s to send to AI)', 'status');
        const sampleRate = 16000;
        audioContext = new AudioContext({ sampleRate });
        const input = audioContext.createMediaStreamSource(stream);
        const bufferSize = 2048;
        processor = audioContext.createScriptProcessor(bufferSize, 1, 1);
        source = input;
        processor.onaudioprocess = (e) => {
          if (!voiceWs || voiceWs.readyState !== WebSocket.OPEN) return;
          const inputData = e.inputBuffer.getChannelData(0);
          const srcRate = audioContext.sampleRate;
          const dstRate = 16000;
          const ratio = srcRate / dstRate;
          const outLength = Math.round(inputData.length / ratio);
          const out = new Int16Array(outLength);
          for (let i = 0; i < outLength; i++) {
            const v = inputData[Math.min(Math.floor(i * ratio), inputData.length - 1)];
            out[i] = Math.max(-32768, Math.min(32767, v * 32767));
          }
          voiceWs.send(out.buffer);
        };
        const gain = audioContext.createGain();
        gain.gain.value = 0;
        input.connect(processor);
        processor.connect(gain);
        gain.connect(audioContext.destination);
      };
      voiceWs.onmessage = (e) => {
        try {
          const d = JSON.parse(e.data);
          if (d.transcript != null) {
            const isFinal = d.isFinal === true;
            setStep(step3, step3Value, d.transcript, isFinal ? 'done' : 'active');
            if (isFinal && d.transcript.trim()) onFinalTranscript(d.transcript.trim());
          }
        } catch (_) {}
      };
      voiceWs.onerror = () => {
        setStep(step2, step2Value, 'WebSocket error', 'status');
        setStep(step3, step3Value, '—', 'status');
      };
      voiceWs.onclose = () => stopVoice();
    });

    stopVoiceBtn.addEventListener('click', () => stopVoice());
  </script>
</body>
</html>
